{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercico Redes Convolucionales con Pytorch\n",
    "\n",
    "Clasificación de imágenes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "import torch\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de constantes \n",
    "import os\n",
    "\n",
    "# esto es para que funcione matplotlib \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 512\n",
    "DATASET_DIR = 'data/DatasetTumoresCerebrales/'\n",
    "MODELS_DIR = 'models/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar en el dataset existe un desbalance en las clases tanto en train como en test.\n",
    "\n",
    "Para solucionar esto aplicaremos oversampling:\n",
    "- Aumentaremos el peso de las clases donde haya menos imágenes.\n",
    "- Aplicaremos DataAugmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomVerticalFlip, ToTensor, RandomRotation, Grayscale\n",
    "import torch.utils.data as data\n",
    "\n",
    "#Aumento de datos\n",
    "transforms = Compose(\n",
    "    [\n",
    "        Grayscale(),\n",
    "        RandomRotation(30),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.ImageFolder(DATASET_DIR+'training',transform= transforms)\n",
    "train, validation = data.random_split(train_data, [0.8, 0.2])\n",
    "testing = datasets.ImageFolder(DATASET_DIR+'test', transform= transforms)\n",
    "\n",
    "#Dado los datos que tenemos del dataset y el desbalance creamos el siguiente sampler\n",
    "sampler = data.WeightedRandomSampler([1/1283, 1/637, 1/837], len(train_data), replacement = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(train, batch_size=BATCH_SIZE, sampler= sampler)\n",
    "val_dataloader = data.DataLoader(validation, batch_size=BATCH_SIZE, sampler= sampler)\n",
    "test_dataloader = data.DataLoader(testing, batch_size=BATCH_SIZE, sampler= sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de Arquitectura\n",
    "\n",
    "### Red Convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "#Bloque convolucional normalizado\n",
    "def bloque_conv_norm(in_features, out_features):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_features, out_features, 4, 1),\n",
    "            nn.BatchNorm2d(out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = bloque_conv_norm(1, 32)\n",
    "        self.conv2 = bloque_conv_norm(32, 64)\n",
    "        self.conv3 = bloque_conv_norm(64, 64)\n",
    "        self.linear_out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(238144, 3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        logits = self.linear_out(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ConvNet (ConvNet)                        [64, 1, 512, 512]    [64, 3]              --                   True\n",
       "├─Sequential (conv1)                     [64, 1, 512, 512]    [64, 32, 254, 254]   --                   True\n",
       "│    └─Conv2d (0)                        [64, 1, 512, 512]    [64, 32, 509, 509]   544                  True\n",
       "│    └─BatchNorm2d (1)                   [64, 32, 509, 509]   [64, 32, 509, 509]   64                   True\n",
       "│    └─ReLU (2)                          [64, 32, 509, 509]   [64, 32, 509, 509]   --                   --\n",
       "│    └─MaxPool2d (3)                     [64, 32, 509, 509]   [64, 32, 254, 254]   --                   --\n",
       "├─Sequential (conv2)                     [64, 32, 254, 254]   [64, 64, 125, 125]   --                   True\n",
       "│    └─Conv2d (0)                        [64, 32, 254, 254]   [64, 64, 251, 251]   32,832               True\n",
       "│    └─BatchNorm2d (1)                   [64, 64, 251, 251]   [64, 64, 251, 251]   128                  True\n",
       "│    └─ReLU (2)                          [64, 64, 251, 251]   [64, 64, 251, 251]   --                   --\n",
       "│    └─MaxPool2d (3)                     [64, 64, 251, 251]   [64, 64, 125, 125]   --                   --\n",
       "├─Sequential (conv3)                     [64, 64, 125, 125]   [64, 64, 61, 61]     --                   True\n",
       "│    └─Conv2d (0)                        [64, 64, 125, 125]   [64, 64, 122, 122]   65,600               True\n",
       "│    └─BatchNorm2d (1)                   [64, 64, 122, 122]   [64, 64, 122, 122]   128                  True\n",
       "│    └─ReLU (2)                          [64, 64, 122, 122]   [64, 64, 122, 122]   --                   --\n",
       "│    └─MaxPool2d (3)                     [64, 64, 122, 122]   [64, 64, 61, 61]     --                   --\n",
       "├─Sequential (linear_out)                [64, 64, 61, 61]     [64, 3]              --                   True\n",
       "│    └─Flatten (0)                       [64, 64, 61, 61]     [64, 238144]         --                   --\n",
       "│    └─Dropout (1)                       [64, 238144]         [64, 238144]         --                   --\n",
       "│    └─Linear (2)                        [64, 238144]         [64, 3]              714,435              True\n",
       "========================================================================================================================\n",
       "Total params: 813,731\n",
       "Trainable params: 813,731\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 203.94\n",
       "========================================================================================================================\n",
       "Input size (MB): 67.11\n",
       "Forward/backward pass size (MB): 13593.84\n",
       "Params size (MB): 3.25\n",
       "Estimated Total Size (MB): 13664.20\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creamos el modelo\n",
    "torch.cuda.empty_cache()\n",
    "model = ConvNet() #ConvNet1 #ConvNet2\n",
    "model.to(device)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# función de entrenamiento\n",
    "def train(dataloader, model, loss_fn, optimizer, losses, accuracy):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            train_acc = 100*correct / size\n",
    "            losses.append(loss.item())\n",
    "            print(f\"Accuracy: {train_acc:>0.1f}%, Avg loss: {loss.item():>8f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_acc = 100*correct/size\n",
    "    accuracy.append(train_acc)\n",
    "\n",
    "def validate(dataloader, model, loss_fn, losses, accuracy):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    val_acc = 100 * correct / size\n",
    "    losses.append(val_loss)\n",
    "    accuracy.append(val_acc)\n",
    "    print(f\"Validation: Accuracy: {val_acc:>0.1f}%, Avg loss: {val_acc:>8f} \\n\")\n",
    "\n",
    "# función de test\n",
    "def test(dataloader, model, loss_fn, losses, accuracy):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    test_acc = 100*correct/size\n",
    "    losses.append(test_loss)\n",
    "    accuracy.append(test_acc)\n",
    "    print(f\"Test Error: \\n Accuracy: {test_acc:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "-------------------------------\n",
      "Accuracy: 1.6%, Avg loss: 0.971739 [   64/ 2206]\n",
      "Validation: Accuracy: 184.0%, Avg loss: 714.872550 \n",
      "\n",
      "Epoch 2/5\n",
      "-------------------------------\n",
      "Accuracy: 2.9%, Avg loss: 0.000000 [   64/ 2206]\n",
      "Validation: Accuracy: 110.7%, Avg loss: 625.700648 \n",
      "\n",
      "Epoch 3/5\n",
      "-------------------------------\n",
      "Accuracy: 2.9%, Avg loss: 0.000000 [   64/ 2206]\n",
      "Validation: Accuracy: 114.2%, Avg loss: 627.368396 \n",
      "\n",
      "Epoch 4/5\n",
      "-------------------------------\n",
      "Accuracy: 2.9%, Avg loss: 0.000000 [   64/ 2206]\n",
      "Validation: Accuracy: 114.0%, Avg loss: 623.724537 \n",
      "\n",
      "Epoch 5/5\n",
      "-------------------------------\n",
      "Accuracy: 2.9%, Avg loss: 0.000000 [   64/ 2206]\n",
      "Validation: Accuracy: 111.6%, Avg loss: 615.748575 \n",
      "\n",
      "Training and Validation Done!\n",
      "Test Error: \n",
      " Accuracy: 502.9%, Avg loss: 821.571333 \n",
      "\n",
      "Test Done!\n",
      "Model saved as model_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Lists to store training and validation results\n",
    "train_loss = []\n",
    "val_loss_list = []\n",
    "test_loss = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc = []\n",
    "\n",
    "for t in range(EPOCHS):\n",
    "    print(f\"Epoch {t + 1}/{EPOCHS}\\n-------------------------------\")\n",
    "    \n",
    "    # Training phase\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current = (batch + 1) * len(X)\n",
    "            train_acc = 100*correct / size\n",
    "            train_loss.append(loss.item())\n",
    "            print(f\"Accuracy: {train_acc:>0.1f}%, Avg loss: {loss.item():>8f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    train_acc = 100*correct/size\n",
    "    \n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    # Validation phase\n",
    "    size = len(val_dataloader.dataset)\n",
    "    num_batches = len(val_dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    val_loss /= num_batches\n",
    "    val_acc = 100 * correct / size\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"Validation: Accuracy: {val_acc:>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "\n",
    "print(\"Training and Validation Done!\")\n",
    "\n",
    "test(test_dataloader, model, loss_fn, test_loss, test_acc)\n",
    "print(\"Test Done!\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODELS_DIR + \"model_cnn.pth\")\n",
    "print(\"Model saved as model_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\TumorClasiffication_Practice\\ClasificacionImagenesTumores.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     plt\u001b[39m.\u001b[39msavefig(\u001b[39m'\u001b[39m\u001b[39maccuracy_plot.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m draw_graphics(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc)\n",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\TumorClasiffication_Practice\\ClasificacionImagenesTumores.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(x, train_loss, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(x, val_loss, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(x, test_loss, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/TumorClasiffication_Practice/ClasificacionImagenesTumores.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend([\u001b[39m'\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTest Loss\u001b[39m\u001b[39m'\u001b[39m], loc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mupper right\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   2813\u001b[0m         \u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39mscalex, scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   2814\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_plot_args(\n\u001b[0;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graphics(train_loss, val_loss, test_loss, train_acc, val_acc, test_acc):\n",
    "    # Dibujamos las gráficas\n",
    "    x = range(len(train_loss))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x, train_loss, color='blue')\n",
    "    plt.plot(x, val_loss, color='green')\n",
    "    plt.plot(x, test_loss, color='red')\n",
    "    plt.legend(['Train Loss', 'Validation Loss', 'Test Loss'], loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('loss_plot.png')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_acc, color='blue')\n",
    "    plt.plot(x, val_acc, color='green')\n",
    "    plt.plot(x, test_acc, color='red')\n",
    "    plt.legend(['Train Accuracy', 'Validation Accuracy', 'Test Accuracy'], loc='lower right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "\n",
    "draw_graphics(train_loss, val_loss_list, test_loss, train_acc_list, val_acc_list, test_acc)\n",
    "train_loss = []\n",
    "val_loss_list = []\n",
    "test_loss = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning: ResNet18 y EfficientNet_V2_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.transforms import Compose, Grayscale\n",
    "import numpy as np\n",
    "\n",
    "# Instanciamos el modelo ResNet18 con los pesos por defecto\n",
    "weights=ResNet18_Weights.DEFAULT\n",
    "auto_transforms = weights.transforms()\n",
    "\n",
    "# Cargamos el dataset con las transformaciones originales\n",
    "transform=Compose(\n",
    "    [\n",
    "        RandomRotation(30),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        auto_transforms,\n",
    "    ]\n",
    "    )\n",
    "\n",
    "training = datasets.MNIST('data', train=True, download=True,\n",
    "                          transform=transform)\n",
    "testing = datasets.MNIST('data', train=False, download=True,\n",
    "                      transform=transform)\n",
    "\n",
    "# Seleccionamos un subconjunto de imágenes de entrenamiento \n",
    "reduced_training = Subset(training, range(2500))\n",
    "\n",
    "train_dataloader = DataLoader(reduced_training, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(testing, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Estructura de los batches\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "# Creamos el modelo y congelamos parámetros de la red\n",
    "model = resnet18(weights=weights).to(device)\n",
    "for name, para in model.named_parameters():\n",
    "    para.requires_grad = False\n",
    "\n",
    "# Cambiamos la capa superior con una capa Lineal con 10 clases de salida\n",
    "num_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_features=num_features,\n",
    "                    out_features=num_classes,\n",
    "                    bias=True).to(device)\n",
    "\n",
    "summary(model,\n",
    "        input_size=(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
